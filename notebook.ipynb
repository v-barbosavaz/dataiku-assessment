{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataiku technical test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interviewee**: Vincent Barbosa Vaz\n",
    "\n",
    "**Dataiku contacts**: Judith Müller, Josh Cooper and Adam Jelley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "  1. [Instructions](#Instructions)\n",
    "- [Load files, wait...?!](#Load-files,-wait...?!)\n",
    "  1. [How to infer columns name ?](#)\n",
    "- [Exploratory analysis](#Exploratory-analysis)\n",
    "  1. [Checking for imbalanced data](#Checking-for-imbalanced-data)\n",
    "  - [More analysis](#More-analysis)\n",
    "  - [Checking for outliers](#Checking-for-outliers)\n",
    "- [Cleaning and feature engineering](#Cleaning-and-feature-engineering)\n",
    "  1. [Data cleaning](#Data-cleaning)\n",
    "  - [Feature engineering](#Feature-engineering)\n",
    "- [Models](#Models)\n",
    "- [Pipeline](#Pipeline)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Sources](#Sources)\n",
    "- [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "The following link lets you download an archive containing an “exercise” US Census dataset:  \n",
    "http://thomasdata.s3.amazonaws.com/ds/us_census_full.zip\n",
    "\n",
    "This US Census dataset contains detailed but anonymized information for approximately 300,000 people.\n",
    "\n",
    "The archive contains 3 files:\n",
    "\n",
    "1. A large training file (csv)\n",
    "2. Another test file (csv)\n",
    "3. A metadata file (txt) describing the columns of the two csv files (identical for both)\n",
    "\n",
    "The goal of this exercise is to model the information contained in the last column (42nd), i.e., whether a person makes more or less than $50,000 per year, from the information contained in the other columns. The exercise here consists of modeling a binary variable.\n",
    "\n",
    "Work with Python (or R) to carry out the following steps:\n",
    "\n",
    "1. Load the train and test files.\n",
    "2. Perform an exploratory analysis on the data and create some relevant visualisations.\n",
    "3. Clean, preprocess, and engineer features in the training data, with the aim of building a data set that a model will perform well on.\n",
    "4. Create a model using these features to predict whether a person earns more or less than $50,000 per year. Here, the idea is for you to test a few different models, and see whether there are any techniques you can apply to improve performance over your first results.\n",
    "5. Choose the model that appears to have the highest performance based on a comparison between reality (the 42nd variable) and the model’s prediction. \n",
    "6. Apply your model to the test file and measure its real performance on it (same method as above).\n",
    "\n",
    "The goal of this exercise is not to create the best or the purest model, but rather to describe the steps you took to accomplish it.  \n",
    "Explain areas that may have been the most challenging for you.  \n",
    "Find clear insights on the profiles of the people that make more than $50,000 / year. For example, which variables seem to be the most correlated with this phenomenon?  \n",
    "Finally, you push your code on GitHub to share it with me, or send it via email.\n",
    "\n",
    "Once again, the goal of this exercise is not to solve this problem, but rather to spend a few hours on it and to thoroughly explain your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files, wait...?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoiler alert, here are the libraries being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "from importlib import reload\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip file contains three files:\n",
    "\n",
    "|File                          |Information                    |\n",
    "|:-----------------------------|:------------------------------|\n",
    "|**census_income_learn.csv**   |Train dataset                  |\n",
    "|**census_income_test.csv**    |Test dataset                   |\n",
    "|**census_income_metadata.txt**|Metadata: feature's information|\n",
    "\n",
    "A first look on **training and test dataset** shows that they **don't have header** (columns names). We will need to get this information from metadata file in order to do feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a Python module to read through metadata file (the code is not very interesting from a ML point of view and it adds some scrolling, that's why it is on a module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_metadata\n",
    "reload(read_metadata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_top = read_metadata.top()\n",
    "meta_top.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_middle = read_metadata.middle()\n",
    "meta_middle.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_bottom = read_metadata.bottom()\n",
    "meta_bottom.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/census_income_learn.csv\", header=None, na_values='?', skipinitialspace=True)\n",
    "names = pd.Series(list(df_train.columns), name='feature').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([names, meta_top.iloc[:,0], meta_middle.iloc[:,0], meta_bottom.iloc[:,0]], axis=1)\n",
    "merged.columns = ['df_train', 'meta_top', 'meta_middle', 'meta_bottom']\n",
    "merged.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Training dataset contains 42 features unnamed.\n",
    "- Features in metadata file are not described consistenly: length of decribed feature vary.\n",
    "- It is not straightforward to infer df_train features names from metadata file.\n",
    "- Features descriptions at the end of metadata file seems to be a better fit (matching length when adding income feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge of features information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(meta_top, meta_middle).merge(meta_bottom).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Feature names vary in file resulting in missing features during merge. An in-depth study would allow us to correctly match all the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to infer columns name ?\n",
    "\n",
    "**Simple idea**\n",
    "\n",
    "As length of feature description at the end of metadata file (bottom file) closely match with the one of training dataset columns we can assume the order is correct and infer.\n",
    "\n",
    "**More complex idea, a path to autoML**\n",
    "\n",
    "To find the right columns names for our datasets one can use information on features in metadata file. We will only focus on the information from the last descriptions of features (bottom file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains unique values for **second column** from **training dataset** we want to find feature name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df_train.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains unique values for feature **class of worker** from **metadata file**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_bottom.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Values are similar\n",
    "- There is a high probability for 2nd column of training dataset to be feature class of worker\n",
    "\n",
    "\n",
    "How to measure \"distance\" between metadata file features and every training dataset column?\n",
    "\n",
    "Jaccard similarity is a distance metric that can be used to measure distance between two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def compute_similarity(index):\n",
    "    similarities = []\n",
    "    for i in range(len(df_train.columns)):\n",
    "        similarities.append([round(jaccard_similarity(list(df_train.iloc[:, i].unique()),\n",
    "                                                      meta_bottom.iloc[index, 1]),2),\n",
    "                             meta_bottom.iloc[index, 0],\n",
    "                             df_train.columns.values[i]\n",
    "                            ])\n",
    "        similarities.sort(reverse=True)\n",
    "\n",
    "    # Select most miningful features\n",
    "    sim = [x[0] for x in similarities]\n",
    "    ind = [i+1 for i,x in enumerate(sim[1:]) if(sim[0]-x<sim[0]-sim[1]+0.05\n",
    "                                                and sim[0]-sim[1] <0.1\n",
    "                                                and sim[0] >0)]\n",
    "    if(sim[0]>0): ind.insert(0, 0)\n",
    "\n",
    "    #print(\"3-closest features:\")\n",
    "    #print(similarities[:5])\n",
    "    #print()\n",
    "    if len(ind)>0:\n",
    "        for i in ind:\n",
    "            print(\"\\\"{}\\\" match \\\"{}\\\" ({}%)\".format(similarities[i][1],\n",
    "                                             similarities[i][2],\n",
    "                                             similarities[i][0]*100))\n",
    "        print()\n",
    "    \n",
    "for i in range(len(meta_bottom)):\n",
    "    compute_similarity(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above measure distances between metadata file and training dataset columns, returning the bests matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We introduced Jaccard similarity to infer information on missing data (columns names). One can create such data to automatically deduce the type of data (autoML for feature engineering).\n",
    "\n",
    "Ultimately we will use the insights given by Jaccard similarity to confirm our pairing.\n",
    "\n",
    "**Pairing method**:\n",
    "\n",
    "1. parse metadata file (last feature descriptions);\n",
    "2. inpute missing column names from parsed data (same order);\n",
    "3. manual validation step with Jaccard similarity;\n",
    "4. construction of names.csv file containing the header for training and test dataset;\n",
    "5. read training and test files setting names attribute with content from names.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load files (with header)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to load our datasets with the corresponding column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\"data/names.csv\", sep=',').columns.tolist()\n",
    "\n",
    "df_train = pd.read_csv(\"data/census_income_learn.csv\", header=None, na_values='?',\n",
    "                       names=names, skipinitialspace=True)\n",
    "\n",
    "df_test = pd.read_csv(\"data/census_income_test.csv\", header=None, na_values='?',\n",
    "                      names=names, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "Performing an exploratory analysis on the data and create some relevant visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "**Meaning of value *'Not in Universe'*:** According to the IPUMS website (https://cps.ipums.org/cps-action/faq), indicates that the census question was irrelevant to the households or persons to whom the question was asked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition of income:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_rep = df_train['income'].value_counts(normalize=True).mul(100).round(2).reset_index()\n",
    "income_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[income_rep['income'][0]],\n",
    "    name=income_rep['index'][0],\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[income_rep['income'][1]],\n",
    "    name=income_rep['index'][1],\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.update_layout(barmode='stack', title = \"Distribution of low and high income\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: *'income'* has two unique values *'50 000+'* and *'- 50 000'* with a ratio 1:9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the distribution of income for *'race'* feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df_train['race'], df_train['income'])\n",
    "crosstab = crosstab.sort_values(crosstab.columns[0],axis=0,ascending=False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(crosstab.columns)):\n",
    "    fig.add_trace(go.Bar(x=crosstab.index, y=crosstab.values[:,i], name=crosstab.columns[i]))\n",
    "    \n",
    "fig.update_layout(\n",
    "    title= \"# of Income vs. Race\",\n",
    "    xaxis_title = \"Race\",\n",
    "    yaxis_title = \"# of Income\",\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Ethnicity is imbalanced, white people are overrepresented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for further plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def countplot(data, x, hue, values=None, aggfunc=None):\n",
    "    \"\"\"\n",
    "    Implementation of sns.countplot for Plotly.\n",
    "    \"\"\"\n",
    "    \n",
    "    crosstab = pd.crosstab(df_train[x], df_train[hue], values=values, aggfunc=aggfunc)\n",
    "    crosstab = crosstab.sort_values(crosstab.columns[0],axis=0,ascending=False)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i in range(len(crosstab.columns)):\n",
    "        fig.add_trace(go.Bar(x=crosstab.index, y=crosstab.values[:,i], name=crosstab.columns[i]))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title= hue.capitalize() + \" vs. \" + x.capitalize() ,\n",
    "        xaxis_title = x,\n",
    "        yaxis_title = hue,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = countplot(data=df_train, x='sex', hue='income')\n",
    "fig.update_layout(\n",
    "    title= '# of Income vs. Sex',\n",
    "    xaxis_title = 'Sex',\n",
    "    yaxis_title = '# of Income',\n",
    "    font=dict(size=12),\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Female and male are balanced (total number of income equal)\n",
    "- Nevertheless, fewer men make less than $50 000 comparing to women"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = countplot(data=df_train, x='sex', hue='income', values=df_train['age'], aggfunc=np.mean)\n",
    "fig.update_layout(\n",
    "    title= 'Average age vs. sex /income',\n",
    "    xaxis_title = 'sex',\n",
    "    yaxis_title = 'average age',\n",
    "    font=dict(size=12),\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Revenue for men and women is unequal\n",
    "- The average age for women making less than $50 000 is 35 while it is 32 for men\n",
    "- Men start earning more money earlier than women do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree map and geographical plots**\n",
    "\n",
    "Let's focus on the \"state of previous residence\" feature.\n",
    "\n",
    "We would like to determine which state most of the high-income earners come from. State of previous residence may be involved in tax income (related to a higher or lower income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_count = pd.crosstab(df_train[\"state of previous residence\"], df_train[\"income\"])\n",
    "states_count = states_count.sort_values(crosstab.columns[0],axis=0,ascending=False)\n",
    "\n",
    "states_count = states_count.reset_index()\n",
    "states_count.rename(columns={'state of previous residence':'state', \n",
    "                             '- 50000.':'under50000', \n",
    "                             '50000+.':'above50000', }, inplace=True)\n",
    "states_count[\"pct\"] = states_count.apply (lambda row: row.above50000/(row.above50000 + row.under50000)*100, axis=1)\n",
    "\n",
    "\n",
    "states_count = states_count[~states_count.state.str.contains(\"Not in universe\")]\n",
    "states_count.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_US = [\"not US\" if (x == 'Abroad') else \"US\" for x in states_count['state']] \n",
    "\n",
    "tree = px.treemap(states_count, \n",
    "                path = [is_US,\"state\"],\n",
    "                values=\"pct\",\n",
    "                color= \"state\")\n",
    "\n",
    "\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous treemap, we group the US states in a same \"box\". Connecticut, New Jersey, Alaska, Massachusetts and Columbia are home to the high income earners (previous residence state). But is there any geographical correlation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states = pd.read_csv(\"data/us_states.csv\")\n",
    "us_states.rename(columns={'State':'state'}, inplace=True)\n",
    "\n",
    "\n",
    "merged = states_count.merge(us_states, on=\"state\")\n",
    "\n",
    "fig = px.choropleth(merged,\n",
    "                    locations=\"Code\",\n",
    "                    color=\"pct\",\n",
    "                    hover_name=\"state\",\n",
    "                    locationmode = 'USA-states')\n",
    "fig.update_layout(\n",
    "    title_text = 'Pourcentage of high income earners - state of their previous residence',\n",
    "    geo_scope='usa',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By mapping the data in a choropleth map, we can see that most of those states are neighbors, and in the East coast of the United States. For a deeper analysis, we can study the migration code-change features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting correlation heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df_train.corr()\n",
    "figure = ff.create_annotated_heatmap(\n",
    "    z=corrs.values,\n",
    "    x=list(corrs.columns),\n",
    "    y=list(corrs.index),\n",
    "    annotation_text=corrs.round(2).values,\n",
    "    showscale=True)\n",
    "figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- As we see above *'weeks worked in year'* and *'detailed industry recode'* **have high correlation** (0.75). *'veterans benefits'* and *'age'* (0.67), *'detailed occupation recode'* and *'weeks worked in year'* (0.66), *'num persons worked for employer'* and *'detailed industry recode'* (0.64) too.\n",
    "\n",
    "- *'detailed industry recode'* and *'detailed occupation recode'* have **medium correlation** between *'veterans benefits'*.\n",
    "\n",
    "- The reminder columns have **low correlation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(\n",
    "    df_train,\n",
    "    dimensions=[\"weeks worked in year\",\n",
    "                \"detailed occupation recode\",\n",
    "                \"capital gains\",\n",
    "                \"num persons worked for employer\",\n",
    "                \"detailed industry recode\",\n",
    "                \"veterans benefits\"\n",
    "               ],\n",
    "    color=\"income\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Scatter plot\",\n",
    "    font=dict(\n",
    "        size=9\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify outliers we will use boxplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(make_subplots(rows=2, cols=2))\n",
    "fig.add_trace(go.Box(y=df_train['age'], name=\"age\"), row=1, col=1)\n",
    "fig.add_trace(go.Box(y=df_train['wage per hour'], name=\"wage per hour\"), row=1, col=2)\n",
    "fig.add_trace(go.Box(y=df_train['capital gains'], name=\"capital gains\"), row=2, col=1)\n",
    "fig.add_trace(go.Box(y=df_train['capital losses'], name=\"capital losses\"), row=2, col=2)\n",
    "fig.update_layout(title='Boxplots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Feature **age** does not have outliers, we will not apply outliers ridding techniques\n",
    "- Other features plots does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-depth analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_train, x=\"race\", y=\"age\", title=\"Box plots\", notched=True, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-depth analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_train, x=\"race\", y=\"age\", color=\"sex\", height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_train, x=\"age\", color=\"sex\", y=\"income\", nbins=50, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_train, x=\"age\", color=\"income\", y=\"sex\", nbins=50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.select_dtypes(include=['number']).isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.select_dtypes(include=['object']).isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- There is no missing data for numerical features\n",
    "- Nearly 100 000 NaN values for *migration code* features\n",
    "\n",
    "We will transform and fill missing values for categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our previous analysis we will remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply winsorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"capital gains\"] = winsorize(df_train[\"capital gains\"],(0,0.035))\n",
    "df_train[\"capital losses\"] = winsorize(df_train[\"capital losses\"],(0,0.019))\n",
    "df_train[\"wage per hour\"] = winsorize(df_train[\"wage per hour\"],(0,0.057))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-plot (with winsorization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(make_subplots(rows=2, cols=2))\n",
    "fig.add_trace(go.Box(y=df_train['age'], name=\"age\"), row=1, col=1)\n",
    "fig.add_trace(go.Box(y=df_train['wage per hour'], name=\"wage per hour\"), row=1, col=2)\n",
    "fig.add_trace(go.Box(y=df_train['capital gains'], name=\"capital gains\"), row=2, col=1)\n",
    "fig.add_trace(go.Box(y=df_train['capital losses'], name=\"capital losses\"), row=2, col=2)\n",
    "fig.update_layout(title='Boxplots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA: numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_num = df_train._get_numeric_data()\n",
    "\n",
    "df_stand = StandardScaler().fit_transform(df_train_num)\n",
    "pca = PCA(n_components=0.9, whiten=True)\n",
    "df_pca = pca.fit_transform(df_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original number of features', df_stand.shape[1])\n",
    "print('Reduced number of features', df_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(y=pca.explained_variance_ratio_,\n",
    "              title='PCA - Total variance explained: {0:.2f}'.format(pca.explained_variance_ratio_.sum()),\n",
    "              height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA: all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_train.drop(columns=['income']).dropna())\n",
    "\n",
    "df_stand = StandardScaler().fit_transform(df_dummies)\n",
    "pca = PCA(n_components=0.9, whiten=True)\n",
    "df_pca = pca.fit_transform(df_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original number of features', df_stand.shape[1])\n",
    "print('Reduced number of features', df_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.line(y=pca.explained_variance_ratio_,\n",
    "              title='PCA - Total variance explained: {0:.2f}'.format(pca.explained_variance_ratio_.sum()),\n",
    "              height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_train.dropna().drop(columns=['income']))\n",
    "y = df_train.dropna()['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 100)\n",
    "model_1 = logreg.fit(X_train, y_train)\n",
    "\n",
    "pred_1 = model_1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_1, target_names = [\"+50 000\", \"-50 000\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randf = RandomForestClassifier(n_estimators = 100)\n",
    "model_2 = randf.fit(X_train, y_train)\n",
    "\n",
    "pred_2 = model_2.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_2, target_names = [\"+50 000\", \"-50 000\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=1)\n",
    "model_3 = gbc.fit(X_train, y_train)\n",
    "\n",
    "pred_3 = model_3.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred_3, target_names = [\"+50 000\", \"-50 000\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\"data/names.csv\", sep=',').columns.tolist()\n",
    "\n",
    "df_train = pd.read_csv(\"data/census_income_learn.csv\", header=None, na_values='?',\n",
    "                       names=names, skipinitialspace=True)\n",
    "\n",
    "df_test = pd.read_csv(\"data/census_income_test.csv\", header=None, na_values='?',\n",
    "                      names=names, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['income'])\n",
    "y = df_train['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('drop_columns', 'drop', ['instance weight']),\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('RandomForestClassifier', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model_pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(columns=['income'])\n",
    "y_test = df_test['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on test dataset even if **we will not use this information to tune our model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Great, columns in test dataset seems to match with header from train dataset. Luckily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.select_dtypes(include=['number']).isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.select_dtypes(include=['object']).isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Same feature repartition for NaNs. Numerical features don't have missing values (our model don't support numerical filling NaNs yet). Luckily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting model with entire train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model_pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columns names, metadata file**\n",
    "\n",
    "It is the first time I encounter datasets with header missing. Usually, metadata files come with training and test sets with further information for context understanding and feature engineering. It gave me the idea to build a matching feature algorithm that could be used for autoML purposes.\n",
    "\n",
    "**Results**\n",
    "\n",
    "We got 0.95 accuracy on test data, impressive! (Did I do something wrong ?)\n",
    "\n",
    "**What to do next?**\n",
    "\n",
    "Compare other ML models, use cross validation etc. Validate the model for production purposes (performances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kdnuggets.com/2018/08/make-machine-learning-models-robust-outliers.html\n",
    "- https://stackoverflow.com/questions/14720324/compute-the-similarity-between-two-lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity implementation to compute distance between two lists. Alternative to Jaccard distance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)\n",
    "\n",
    "def cosine(listA, df):\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(len(df.columns)): \n",
    "        counterA = Counter(listA[1])\n",
    "        counterB = Counter(list(df.iloc[:, i].unique()))\n",
    "        \n",
    "        sim = counter_cosine_similarity(counterA, counterB)\n",
    "        similarities.append([round(sim, 2), listA[0], df.columns.values[i]])\n",
    "        similarities.sort(reverse=True)\n",
    "    return similarities\n",
    "\n",
    "cosine(meta_bottom.iloc[1, :], df_train)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to [Crosstab](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html) is [Pivot Table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html). For this purpose Crosstab has a more succinct syntax. These two lines are equivalent:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "crosstab = pd.crosstab(df_train['race'], df_train['income'])\n",
    "pivot = pd.pivot_table(df_train[['income', 'race']], index='race', columns='income', aggfunc=len, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly alternative to plot histogram categorical variables:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x0 = df_train[df_train[\"income\"]==\"50000+.\"][\"race\"]\n",
    "x1 = df_train[df_train[\"income\"]==\"- 50000.\"][\"race\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0))\n",
    "fig.add_trace(go.Histogram(x=x1))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly alternative to plot histogram:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=df_train[df_train[\"income\"]==\"- 50000.\"][\"age\"]))\n",
    "fig.add_trace(go.Histogram(x=df_train[df_train[\"income\"]==\"50000+.\"][\"age\"]))\n",
    "\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly static:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_bytes = fig.to_image(format=\"png\")\n",
    "from IPython.display import Image\n",
    "Image(img_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
